{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [],
   "source": [
    "import math, sys\n",
    "from konlpy.tag import Okt\n",
    "\n",
    "class BayesianFilter:\n",
    "  \"\"\" 베이지안 필터 \"\"\"\n",
    "  def __init__(self):\n",
    "    self.words = set() # 출현 단어 기록 \n",
    "    self.word_dict = {} # 카테고리 마다 출현 횟수 기록\n",
    "    self.category_dict = {} # 카테고리 출현 횟수 기록 \n",
    "\n",
    "    # 형태소 분석하기 \n",
    "  def split(self, text): \n",
    "    results = []\n",
    "    okt = Okt() \n",
    "    # 단어의 기본형 사용\n",
    "    malist = okt.pos(text, norm=True, stem=True) \n",
    "    for word in malist: \n",
    "        # 어미/조사/구두점 등을 대상에서 제외  \n",
    "      if not word[1] in [\"Josa\",\"Eomi\",\"Punctuation\"]:\n",
    "        results.append(word[0]) \n",
    "    return results \n",
    "\n",
    "    # 단어와 카테고리의 출현 횟수 세기\n",
    "  def inc_word(self, word, category):\n",
    "    # 단어를 카테고리에 추가하기\n",
    "    if not category in self.word_dict: \n",
    "      self.word_dict[category] = {} \n",
    "    if not word in self.word_dict[category]: \n",
    "      self.word_dict[category][word] = 0\n",
    "      self.word_dict[category][word] += 1 \n",
    "      self.words.add(word) \n",
    "\n",
    "  def inc_category(self, category):\n",
    "    # 카테고리 계산\n",
    "    if not category in self.category_dict: \n",
    "      self.category_dict[category] = 0\n",
    "      self.category_dict[category] += 1 \n",
    "\n",
    "  # 텍스트 학습하기\n",
    "  def fit(self, text, category):\n",
    "    \"\"\" 텍스트 학습 \"\"\" \n",
    "    word_list = self.split(text) \n",
    "    for word in word_list:\n",
    "      self.inc_word(word, category) \n",
    "    self.inc_category(category)  \n",
    "  \n",
    "  # 단어 리스트에 점수 매기기 \n",
    "  def score(self, words, category):\n",
    "    score = math.log(self.category_prob(category)) \n",
    "    for word in words:\n",
    "      score += math.log(self.word_prob(word, category)) \n",
    "    return score\n",
    "\n",
    "  # 카테고리 내부의 단어 출현 횟수 구하기\n",
    "  def get_word_count(self, word, category): \n",
    "    if word in self.word_dict[category]:\n",
    "      return self.word_dict[category][word]\n",
    "    else:\n",
    "      return 0\n",
    "\n",
    "  def category_prob(self, category): \n",
    "    \"\"\" 카테고리 계산 \"\"\" \n",
    "    sum_categories = sum(self.category_dict.values()) \n",
    "    category_v = self.category_dict[category] \n",
    "    return category_v / sum_categories \n",
    "\n",
    "  def word_prob(self, word, category):\n",
    "    \"\"\" 카테고리 내부의 단어 출현 비율을 계산 \"\"\"\n",
    "    n = self.get_word_count(word, category) + 1 # 0은 의미가 없으므로 +1을 해야한다.\n",
    "    d = sum(self.word_dict[category].values()) + len(self.words) \n",
    "    return n / d \n",
    "\n",
    "  def predict(self, text): \n",
    "    best_category = None\n",
    "    max_score = -sys.maxsize \n",
    "    words = self.split(text) \n",
    "    score_list = [] \n",
    "    for category in self.category_dict.keys():\n",
    "      score = self.score(words, category)\n",
    "      score_list.append((category, score))\n",
    "      if score > max_score:\n",
    "        max_score = score\n",
    "        best_category = category \n",
    "    return best_category, score_list \n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# from bayes import BayesianFilter => 모듈을 호출하여 BayesianFilter를 로드... "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [],
   "source": [
    "bf = BayesianFilter() # 클래스 호출(객체 생성) \n",
    "\n",
    "## 텍스트 학습\n",
    "bf.fit(\"파격 세일 - 오늘까지만 30%할인\", \"광고\") \n",
    "bf.fit(\"봄과 함께 찾아온 따뜻한 신제품 소식\", \"광고\") \n",
    "bf.fit(\"인기 제품 기간 한정 세일\", \"광고\") \n",
    "bf.fit(\"쿠폰 선물 & 무료 배송\", \"광고\") \n",
    "bf.fit(\"신세계 백화점 세일\", \"광고\")\n",
    "bf.fit(\"오늘 일정 확인\", \"중요\")\n",
    "bf.fit(\"프로젝트 진행 상황 보고\", \"중요\")\n",
    "bf.fit(\"계약 잘 부탁드립니다\", \"중요\")\n",
    "bf.fit(\"회의 일정이 등록되었습니다.\", \"중요\") \n",
    "bf.fit(\"오늘 일정이 없습니다.\", \"중요\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "결과 예측 = 중요\n",
      "[('광고', -20.21525633917275), ('중요', -18.072807129433244)]\n"
     ]
    }
   ],
   "source": [
    "# 예측 \n",
    "pre, score_list = bf.predict(\"계약은 오늘 회의에서 결정됩니다\") \n",
    "print(\"결과 예측 =\", pre) \n",
    "print(score_list)  "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import os, glob, json\n",
    "root_dir = \"./newstext\"\n",
    "dic_file = root_dir + \"/word-dic.json\"\n",
    "data_file = root_dir + \"/data.json\" \n",
    "data_file_min = root_dir + \"/data-mini.json\" \n",
    "\n",
    "## 어구를 자르고 ID로 변환\n",
    "word_dic = {\"_MAX\": 0} \n",
    "def text_to_ids(text): \n",
    "  text = text.strip()\n",
    "  words = text.split(\" \")\n",
    "  result = []\n",
    "  for n in words:\n",
    "    n = n.strip()\n",
    "    if n == \"\" : continue\n",
    "    if not n in word_dic:\n",
    "      wid = word_dic[n] = word_dic[\"_MAX\"] \n",
    "      word_dic[\"_MAX\"] += 1\n",
    "      print(wid, n)\n",
    "    else:\n",
    "      wid = word_dic[n]\n",
    "    result.append(wid) \n",
    "  return result # 단어의 사용 횟수를 알수 있다. \n",
    "\n",
    "# 파일을 읽고 고정 길이의 배열 리턴하기  \n",
    "def file_to_ids(fname):\n",
    "  with open(fname, \"r\", encoding=\"utf-8\") as f: \n",
    "    text = f.read()\n",
    "  return text_to_ids(text)\n",
    "\n",
    "# 딕셔너리에 단어 모두 등록하기\n",
    "def register_dic():\n",
    "  files = glob.glob(root_dir+\"/*/*.wakati\", recursive=True) \n",
    "  for i in files:\n",
    "    file_to_ids(i)\n",
    "\n",
    "# 파일 내부의 단어 세기\n",
    "def count_file_freq(fname):\n",
    "  cnt = [0 for n in range(word_dic[\"_MAX\"])] \n",
    "  with open(fname,\"r\", encoding=\"utf-8\") as f:\n",
    "    text = f.read().strip() \n",
    "    ids = text_to_ids(text) \n",
    "    for wid in ids:\n",
    "      cnt[wid] += 1 \n",
    "  return cnt\n",
    "\n",
    "# 카테고리마다 파일 읽어 들이기 \n",
    "def count_freq(limit = 0): \n",
    "  X = []\n",
    "  Y = []\n",
    "  max_words = word_dic[\"_MAX\"]\n",
    "  cat_names = []\n",
    "  for cat in os.listdir(root_dir):\n",
    "    cat_dir = root_dir + \"/\" + cat\n",
    "    if not os.path.isdir(cat_dir): continue # 내용이 없다면\n",
    "    cat_idx = len(cat_names)  \n",
    "    cat_names.append(cat)\n",
    "    files = glob.glob(cat_dir+\"/*.wakati\") \n",
    "    i = 0\n",
    "    for path in files: \n",
    "      print(path)\n",
    "      cnt = count_file_freq(path) \n",
    "      X.append(cnt)\n",
    "      Y.append(cat_idx) \n",
    "      if limit > 0:\n",
    "        if i > limit: break\n",
    "        i += 1 \n",
    "  return X,Y\n",
    "\n",
    "# 단어 딕셔너리 만들기 \n",
    "if os.path.exists(dic_file):\n",
    "  word_dic =json.load(open(dic_file)) \n",
    "else:\n",
    "  register_dic()\n",
    "  json.dump(word_dic, open(dic_file,\"w\",encoding=\"utf-8\"))\n",
    "\n",
    "# 벡터를파일로출력하기 \n",
    "# 테스트 목적의 소규모 데이터 만들기\n",
    "X, Y = count_freq(20)\n",
    "json.dump({\"X\": X, \"Y\": Y}, open(data_file_min,\"w\",encoding=\"utf-8\"))\n",
    "# 전체 데이터를 기반으로 데이터 만들기\n",
    "X, Y = count_freq()\n",
    "json.dump({\"X\": X, \"Y\": Y}, open(data_file,\"w\",encoding=\"utf-8\")) \n",
    "print(\"ok\")\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "### keras를 이용한 MLP 텍스트 분류하기\n",
    "from keras.models import Sequential \n",
    "from keras.layers import Dense, Dropout, Activation \n",
    "from keras.wrappers.scikit_learn import KerasClassifier \n",
    "from keras.utils import np_utils\n",
    "from sklearn.model_selection import train_test_split\n",
    "from sklearn import model_selection, metrics\n",
    "import json   \n",
    "\n",
    "max_words = 77238\n",
    "nb_classes = 6 # 9개의 카테고리\n",
    "\n",
    "batch_size = 64 \n",
    "nb_epoch = 20\n",
    "\n",
    "# MLP 모델 생성하기 \n",
    "def build_model():\n",
    "  model = Sequential()\n",
    "  model.add(Dense(512, input_shape=(max_words,))) \n",
    "  model.add(Activation('relu'))\n",
    "  model.add(Dropout(0.5)) \n",
    "  model.add(Dense(nb_classes))\n",
    "  model.add(Activation('softmax'))\n",
    "  model.compile(loss='categorical_crossentropy',\n",
    "                optimizer='adam',\n",
    "                metrics=['accuracy']) \n",
    "  return model\n",
    "\n",
    "# 데이터 읽어 들이기 \n",
    "data = json.load(open(\"./newstext/data-mini.json\")) \n",
    "#data = json.load(open(\"./newstext/data.json\"))\n",
    "X = data[\"X\"] \n",
    "Y = data[\"Y\"] \n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# 학습하기 \n",
    "X_train, X_test, Y_train, Y_test = train_test_split(X, Y) \n",
    "Y_train = np_utils.to_categorical(Y_train, nb_classes) \n",
    "print(len(X_train),len(Y_train))\n",
    "model = KerasClassifier( \n",
    "  build_fn=build_model, \n",
    "  nb_epoch=nb_epoch, \n",
    "  batch_size=batch_size)\n",
    "model.fit(X_train, Y_train)\n",
    "\n",
    "#예측하기 \n",
    "y = model.predict(X_test)\n",
    "ac_score = metrics.accuracy_score(Y_test, y) \n",
    "cl_report = metrics.classification_report(Y_test, y) \n",
    "print(\"정답률 =\", ac_score)\n",
    "print(\"리포트 =\\n\", cl_report)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "2\n",
      "0 신촌역\n",
      "1 신천역\n",
      "2 신천군\n",
      "2 신발\n",
      "2 마곡역\n"
     ]
    }
   ],
   "source": [
    "### 레벤슈타인 거리 구하는 프로그램 \n",
    "def calc_distance(a, b):\n",
    "  \"\"\" 레벤슈타인 거리 계산하기 \"\"\"\n",
    "  if a == b: return 0\n",
    "  a_len = len(a) \n",
    "  b_len = len(b)\n",
    "  if a == \"\": return b_len \n",
    "  if b == \"\": return a_len\n",
    "\n",
    "#2차원표(a_len+1,b_len+1)\n",
    "  matrix = [[] for i in range(a_len+1)]\n",
    "  for i in range(a_len+1): # 0으로 초기화\n",
    "    matrix[i] = [0 for j in range(b_len+1)] # 0일 때 초깃값을 설정\n",
    "\n",
    "  for i in range(a_len+1):\n",
    "    matrix[i][0] = i\n",
    "  for j in range(b_len+1): matrix[0][j] = j\n",
    "\n",
    "#표 채우기 \n",
    "  for i in range(1, a_len+1):\n",
    "    ac = a[i-1]\n",
    "    for j in range(1, b_len+1):\n",
    "      bc = b[j-1]\n",
    "      cost = 0 if (ac == bc) else 1\n",
    "      matrix[i][j] = min ([\n",
    "        matrix[i-1][j] + 1, # 문자 삽입 \n",
    "        matrix[i][j-1] + 1, # 문자 제거 \n",
    "        matrix[i-1][j-1] + cost # 문자 변경\n",
    "\n",
    "      ])\n",
    "  return matrix[a_len][b_len]\n",
    "# \"가나다라\"와 \"가마바라\"의 거리\n",
    "print(calc_distance(\"가나다라\", \"가마바라\"))\n",
    "# 실행 예\n",
    "samples = [\"신촌역\",\"신천군\",\"신천역\",\"신발\",\"마곡역\"]\n",
    "base = samples[0]\n",
    "r = sorted(samples, key = lambda n: calc_distance(base, n)) \n",
    "for n in r:\n",
    "  print(calc_distance(base, n), n)\n",
    "\n",
    "\n",
    " "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "interpreter": {
   "hash": "aee8b7b246df8f9039afb4144a1f6fd8d2ca17a180786b69acc140d282b71a49"
  },
  "kernelspec": {
   "display_name": "Python 3.10.3 64-bit",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.6"
  },
  "orig_nbformat": 4
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
